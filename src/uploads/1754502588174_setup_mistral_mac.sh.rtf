{\rtf1\ansi\ansicpg1252\cocoartf2822
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\paperw11900\paperh16840\margl1440\margr1440\vieww11520\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 #!/bin/bash\
\
# Set up variables\
MODEL_URL="https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf"\
MODEL_DIR="models"\
MODEL_NAME="mistral-7b-v0.1.Q4_K_M.gguf"\
\
echo "Installing prerequisites..."\
/bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)" || true\
brew install cmake git wget || true\
\
echo "Cloning llama.cpp..."\
git clone https://github.com/ggerganov/llama.cpp.git || true\
cd llama.cpp\
\
echo "Building llama.cpp with Metal support..."\
make LLAMA_METAL=1\
\
echo "Creating model directory and downloading Mistral model..."\
mkdir -p $MODEL_DIR\
cd $MODEL_DIR\
wget -c $MODEL_URL -O $MODEL_NAME\
cd ..\
\
echo "Running Mistral model with Metal acceleration..."\
./main -m $MODEL_DIR/$MODEL_NAME -p "What is the future of AI?" -ngl 1\
}